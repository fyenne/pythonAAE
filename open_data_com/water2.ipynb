{"metadata":{"kernelspec":{"name":"python386jvsc74a57bd0ff684bfe6bac5aee95fe25613146015247c6b1b77f36ee2373fa7468888d9e66","display_name":"Python 3.8.6 64-bit ('siming': conda)"},"language_info":{"name":"python","version":"3.8.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"ce909994499a62a89f8945d91d57c51a920a5cb8f3e61b9dc99d2a0ea5862498"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[]},"outputs":[],"source":["# pip install xgboost --no-binary xgboost -v"]},{"cell_type":"code","source":["train = pd.read_csv('/kaggle/input/may5aaa/55data_train.csv')\n","tests = pd.read_csv('/kaggle/input/may5aaa/55data_tests.csv')"],"metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/kaggle/input/raw-water/raw_train.csv')\n","tests = pd.read_csv('/kaggle/input/raw-water/raw_tests.csv')"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels=train.pop('AåŽ‚')\n","train_prepared=train"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mlxtend.regressor import StackingRegressor\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.ensemble import GradientBoostingRegressor"],"metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import time #implementing in this function the time spent on training the model\n","from sklearn import metrics\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import cross_val_score, KFold\n","\n","def modelfit(alg, dtrain, target, only_predict = False):\n","    #Fit the algorithm on the data\n","    time_start = time.perf_counter() #start counting the time\n","    if not only_predict:\n","        alg.fit(dtrain, target)\n","        \n","    #Predict training set:\n","    dtrain_predictions = alg.predict(dtrain)\n","    \n","    kfolds = KFold(n_splits=6, shuffle=True, random_state=42)\n","    \n","    cv_score = cross_val_score(alg, dtrain,target, cv=kfolds, scoring='neg_mean_squared_error')\n","    cv_score = np.sqrt(-cv_score)\n","    \n","    time_end = time.perf_counter()\n","    \n","    total_time = time_end-time_start\n","    #Print model report:\n","    print(\"\\nModel Report\")\n","    print(\"RMSE :  {:.4f}\".format(np.sqrt(mean_squared_error(target, dtrain_predictions))))\n","    print(\"CV Score : Mean -  %.4f | Std -  %.4f | Min -  %.4f | Max - %.4f\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n","    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))\n","    \n","def plot_feature_importance(model, df):\n","    feature_importance = model.feature_importances_[:30]\n","    # make importances relative to max importance\n","    plt.figure(figsize=(20, 20)) #figure size\n","    feature_importance = 100.0 * (feature_importance / feature_importance.max()) #making it a percentage relative to the max value\n","    sorted_idx = np.argsort(feature_importance)\n","    pos = np.arange(sorted_idx.shape[0]) + .5\n","    plt.barh(pos, feature_importance[sorted_idx], align='center')\n","    plt.yticks(pos, df.columns[sorted_idx], fontsize=15) #used train_drop here to show the name of each feature instead of our train_prepared \n","    plt.xlabel('Relative Importance', fontsize=20)\n","    plt.ylabel('Features', fontsize=20)\n","    plt.title('Variable Importance', fontsize=30)"],"metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["lin_reg = LinearRegression()\n","modelfit(lin_reg, train_prepared, train_labels)"],"metadata":{"trusted":true},"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_prepared' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8e02120f2423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlin_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_prepared' is not defined"]}]},{"cell_type":"code","source":["forest_reg = RandomForestRegressor(n_estimators=300, \n","                                   random_state=1026, \n","                                   min_samples_leaf=5,\n","                                   min_samples_split = 5,\n","                                   max_depth = 15,\n","                                   n_jobs=-1, oob_score=True)\n","modelfit(forest_reg, train_prepared, train_labels)"],"metadata":{"trusted":true},"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_prepared' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a35d9b347d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    n_jobs=-1, oob_score=True)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_prepared' is not defined"]}]},{"cell_type":"code","source":["plot_feature_importance(forest_reg, train_prepared)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg = DecisionTreeRegressor(random_state=421, max_depth = 15, min_samples_split = 5)\n","modelfit(tree_reg, train_prepared, train_labels) "],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostRegressor\n","\n","# tree_ada = DecisionTreeRegressor(random_state = 42,max_depth = 4)\n","\n","ada_reg = AdaBoostRegressor(\n","    tree_reg, n_estimators=300, random_state=42,learning_rate=0.009, loss='square')\n","modelfit(ada_reg, train_prepared, train_labels)\n","\n","# 181.589"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_feature_importance(ada_reg, train_prepared)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params = {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 2,\n","          'learning_rate': 0.01, 'loss': 'ls'}\n","\n","params_1 = {'n_estimators': 1000, 'learning_rate': 0.05, 'max_depth' : 14,\n","            'max_features': 'sqrt', 'min_samples_leaf': 15, 'min_samples_split': 5, \n","            'loss': 'huber', 'random_state': 42}\n","\n","# train_dummies_prepared = num_pipeline.fit_transform(train_dummies)\n","\n","gdb_model = GradientBoostingRegressor(**params_1)\n","modelfit(gdb_model, train_prepared, train_labels)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","# now "],"metadata":{}},{"cell_type":"code","source":["train = pd.read_csv('/kaggle/input/may5aaa/55data_train.csv')\n","tests = pd.read_csv('/kaggle/input/may5aaa/55data_tests.csv')"],"metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_raw2 = train\n","tests_raw2 = tests"],"metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["train_raw2['BåŽ‚'] = train_raw2['BåŽ‚'].astype(int) \n","tests_raw2['BåŽ‚'] = tests_raw2['BåŽ‚'].astype(int) \n","# for i in ['BåŽ‚', 'year', 'wee']:\n","#     train_raw2[i] = train_raw2[i].astype(object)\n","#     tests_raw2[i] = tests_raw2[i].astype(object)\n","\n","tests_raw2.columns = train_raw2.columns"],"metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train_raw2.info()"],"metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2070 entries, 0 to 2069\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   AåŽ‚      2070 non-null   float64\n 1   BåŽ‚      2070 non-null   int64  \n 2   year    2070 non-null   int64  \n 3   month   2070 non-null   int64  \n 4   date    2070 non-null   int64  \n 5   wee     2070 non-null   int64  \n 6   sum     2070 non-null   float64\n 7   sub     2070 non-null   float64\n 8   div     2070 non-null   float64\n 9   log     2070 non-null   float64\ndtypes: float64(5), int64(5)\nmemory usage: 161.8 KB\n","output_type":"stream"}]},{"cell_type":"code","source":["y = train_raw2.pop('AåŽ‚')\n","X = train_raw2\n","tests_raw2.pop('AåŽ‚')"],"metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0      1.926031\n1      1.926081\n2      1.926131\n3      1.926180\n4      1.926230\n         ...   \n297   -0.299085\n298   -0.299056\n299   -0.299028\n300   -0.298999\n301   -0.298970\nName: AåŽ‚, Length: 302, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":["y = np.array(y).reshape(-1,1)"],"metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["tests_raw2.info()"],"metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 302 entries, 0 to 301\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   BåŽ‚      302 non-null    int64  \n 1   year    302 non-null    int64  \n 2   month   302 non-null    int64  \n 3   date    302 non-null    int64  \n 4   wee     302 non-null    int64  \n 5   sum     302 non-null    float64\n 6   sub     302 non-null    float64\n 7   div     302 non-null    float64\n 8   log     302 non-null    float64\ndtypes: float64(4), int64(5)\nmemory usage: 21.4 KB\n","output_type":"stream"}]},{"cell_type":"code","source":["meta_random_seed = 1026529"],"metadata":{"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import optuna\n","import lightgbm as lgb\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","\n","from sklearn.metrics import mean_absolute_percentage_error"],"metadata":{"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":["def objective(trial, X=X, y=y):\n","\n","    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=meta_random_seed)\n","\n","\n","    lgb_params={\n","        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n","        'max_depth': trial.suggest_int('max_depth', 6, 31),\n","        'num_leaves': trial.suggest_int('num_leaves', 31, 120),\n","        'reg_alpha': trial.suggest_float('reg_alpha', 1e-1, 1),\n","        'reg_lambda': trial.suggest_float('reg_lambda', 1e-1, 1),\n","        'random_state': meta_random_seed,\n","        'metric': 'mse',\n","        'n_estimators': trial.suggest_int('n_estimators', 6, 3000),\n","        'n_jobs': -1,\n","        'cat_feature': [x for x in range(len(categorical_columns))],\n","        'bagging_seed': 2021,\n","        'feature_fraction_seed': 2021,\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),\n","        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n","        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n","        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n","        'min_data_per_group': trial.suggest_int('min_data_per_group', 5, 50),\n","        'cat_smooth': trial.suggest_int('cat_smooth', 10, 250),\n","        'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n","    }\n","\n","    lgb = LGBMRegressor(\n","        **lgb_params\n","    )\n","    lgb.fit(\n","        X_train,\n","        y_train,\n","        eval_set=(X_test,y_test),\n","        eval_metric='mean_absolute_percentage_error',\n","        early_stopping_rounds=100,\n","        verbose=False\n","    )\n","    predictions=lgb.predict(X_test)\n","\n","    return mean_absolute_percentage_error(y_test, predictions)"],"metadata":{"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["study = optuna.create_study(direction='minimize')\n","study.optimize(objective, timeout=3600*7, n_trials=25)"],"metadata":{"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2021-05-05 06:57:42,005]\u001b[0m A new study created in memory with name: no-name-75e64e44-ecaa-4fd5-8827-9d5dc3a2cab8\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:43,396]\u001b[0m Trial 0 finished with value: 0.8466679394775066 and parameters: {'learning_rate': 0.001742788591210247, 'max_depth': 143, 'num_leaves': 78, 'reg_alpha': 0.9133738623181126, 'reg_lambda': 0.8756985578508477, 'n_estimators': 2837, 'colsample_bytree': 0.7033306741101055, 'min_child_samples': 106, 'subsample_freq': 3, 'subsample': 0.7421045833839981, 'max_bin': 807, 'min_data_per_group': 34, 'cat_smooth': 89, 'cat_l2': 13}. Best is trial 0 with value: 0.8466679394775066.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:44,990]\u001b[0m Trial 1 finished with value: 0.6927860471694943 and parameters: {'learning_rate': 0.008450622894355915, 'max_depth': 132, 'num_leaves': 66, 'reg_alpha': 0.3667392141007241, 'reg_lambda': 0.6429189662447613, 'n_estimators': 2907, 'colsample_bytree': 0.8645515152117613, 'min_child_samples': 109, 'subsample_freq': 8, 'subsample': 0.8043580474203871, 'max_bin': 736, 'min_data_per_group': 41, 'cat_smooth': 190, 'cat_l2': 20}. Best is trial 1 with value: 0.6927860471694943.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:45,185]\u001b[0m Trial 2 finished with value: 2.414622191525657 and parameters: {'learning_rate': 0.004967962988877657, 'max_depth': 120, 'num_leaves': 50, 'reg_alpha': 0.5620086082936988, 'reg_lambda': 0.9822263299732077, 'n_estimators': 649, 'colsample_bytree': 0.8442009871177183, 'min_child_samples': 313, 'subsample_freq': 6, 'subsample': 0.8818708028013604, 'max_bin': 622, 'min_data_per_group': 29, 'cat_smooth': 33, 'cat_l2': 16}. Best is trial 1 with value: 0.6927860471694943.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:46,881]\u001b[0m Trial 3 finished with value: 0.6173058425355605 and parameters: {'learning_rate': 0.005512632205979062, 'max_depth': 125, 'num_leaves': 77, 'reg_alpha': 0.8267815365565572, 'reg_lambda': 0.4192139290930603, 'n_estimators': 2803, 'colsample_bytree': 0.8773096117527946, 'min_child_samples': 82, 'subsample_freq': 9, 'subsample': 0.7602649563170206, 'max_bin': 593, 'min_data_per_group': 46, 'cat_smooth': 153, 'cat_l2': 2}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:48,923]\u001b[0m Trial 4 finished with value: 0.8197414649355238 and parameters: {'learning_rate': 0.007383857070276503, 'max_depth': 14, 'num_leaves': 97, 'reg_alpha': 0.5377631266234949, 'reg_lambda': 0.8649889743833714, 'n_estimators': 1698, 'colsample_bytree': 0.8776657652740885, 'min_child_samples': 97, 'subsample_freq': 4, 'subsample': 0.7395901211657625, 'max_bin': 623, 'min_data_per_group': 19, 'cat_smooth': 27, 'cat_l2': 18}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:49,326]\u001b[0m Trial 5 finished with value: 0.8168001560232461 and parameters: {'learning_rate': 0.00510112632783748, 'max_depth': 118, 'num_leaves': 37, 'reg_alpha': 0.5469670519984087, 'reg_lambda': 0.36822457515188856, 'n_estimators': 996, 'colsample_bytree': 0.8555634590296942, 'min_child_samples': 166, 'subsample_freq': 2, 'subsample': 0.7978548169259354, 'max_bin': 1024, 'min_data_per_group': 39, 'cat_smooth': 151, 'cat_l2': 20}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:49,351]\u001b[0m Trial 6 finished with value: 1.2310289394596292 and parameters: {'learning_rate': 0.00462656445697578, 'max_depth': 25, 'num_leaves': 82, 'reg_alpha': 0.42460320615638236, 'reg_lambda': 0.7193074754478105, 'n_estimators': 37, 'colsample_bytree': 0.7696803110603915, 'min_child_samples': 484, 'subsample_freq': 1, 'subsample': 0.8208965719650577, 'max_bin': 266, 'min_data_per_group': 18, 'cat_smooth': 70, 'cat_l2': 17}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:49,624]\u001b[0m Trial 7 finished with value: 3.1581752035012682 and parameters: {'learning_rate': 0.008517206591472828, 'max_depth': 153, 'num_leaves': 87, 'reg_alpha': 0.22440885480642372, 'reg_lambda': 0.12936541420737543, 'n_estimators': 1183, 'colsample_bytree': 0.8508686722967904, 'min_child_samples': 420, 'subsample_freq': 7, 'subsample': 0.7900226369883505, 'max_bin': 335, 'min_data_per_group': 33, 'cat_smooth': 43, 'cat_l2': 2}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:50,624]\u001b[0m Trial 8 finished with value: 0.8101249661007802 and parameters: {'learning_rate': 0.003551989924506857, 'max_depth': 72, 'num_leaves': 79, 'reg_alpha': 0.5639254085551724, 'reg_lambda': 0.11521045796304032, 'n_estimators': 1961, 'colsample_bytree': 0.8121887939376845, 'min_child_samples': 109, 'subsample_freq': 1, 'subsample': 0.7895862143273172, 'max_bin': 444, 'min_data_per_group': 44, 'cat_smooth': 53, 'cat_l2': 8}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:50,940]\u001b[0m Trial 9 finished with value: 3.3697721131015137 and parameters: {'learning_rate': 0.005427968035858881, 'max_depth': 228, 'num_leaves': 82, 'reg_alpha': 0.4340873749502887, 'reg_lambda': 0.332900007720902, 'n_estimators': 1712, 'colsample_bytree': 0.8438374560906043, 'min_child_samples': 459, 'subsample_freq': 6, 'subsample': 0.7708085845446159, 'max_bin': 746, 'min_data_per_group': 35, 'cat_smooth': 70, 'cat_l2': 18}. Best is trial 3 with value: 0.6173058425355605.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:54,442]\u001b[0m Trial 10 finished with value: 0.4920267254953815 and parameters: {'learning_rate': 0.0007000386887954261, 'max_depth': 208, 'num_leaves': 120, 'reg_alpha': 0.9622979235376654, 'reg_lambda': 0.4266258726746097, 'n_estimators': 2398, 'colsample_bytree': 0.7540387555164995, 'min_child_samples': 6, 'subsample_freq': 10, 'subsample': 0.7130583056296598, 'max_bin': 998, 'min_data_per_group': 50, 'cat_smooth': 232, 'cat_l2': 2}. Best is trial 10 with value: 0.4920267254953815.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:57:57,589]\u001b[0m Trial 11 finished with value: 0.6618216094920334 and parameters: {'learning_rate': 0.00041583703539812194, 'max_depth': 212, 'num_leaves': 119, 'reg_alpha': 0.9821186806754643, 'reg_lambda': 0.4314076454276288, 'n_estimators': 2413, 'colsample_bytree': 0.739420902614228, 'min_child_samples': 11, 'subsample_freq': 10, 'subsample': 0.7081852779937952, 'max_bin': 1000, 'min_data_per_group': 50, 'cat_smooth': 246, 'cat_l2': 1}. Best is trial 10 with value: 0.4920267254953815.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:02,181]\u001b[0m Trial 12 finished with value: 0.3265924903581929 and parameters: {'learning_rate': 0.0026955685638868463, 'max_depth': 180, 'num_leaves': 110, 'reg_alpha': 0.8365820878758264, 'reg_lambda': 0.5052568831768142, 'n_estimators': 2458, 'colsample_bytree': 0.7589370718114493, 'min_child_samples': 7, 'subsample_freq': 10, 'subsample': 0.7036373549101399, 'max_bin': 906, 'min_data_per_group': 50, 'cat_smooth': 242, 'cat_l2': 4}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:04,783]\u001b[0m Trial 13 finished with value: 0.8167588332457627 and parameters: {'learning_rate': 0.00019510489988737749, 'max_depth': 190, 'num_leaves': 120, 'reg_alpha': 0.759422654539037, 'reg_lambda': 0.5760998575745401, 'n_estimators': 2350, 'colsample_bytree': 0.74055952434404, 'min_child_samples': 23, 'subsample_freq': 10, 'subsample': 0.709909760031551, 'max_bin': 886, 'min_data_per_group': 5, 'cat_smooth': 248, 'cat_l2': 6}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:05,524]\u001b[0m Trial 14 finished with value: 1.3613603682146527 and parameters: {'learning_rate': 0.0020557900283468236, 'max_depth': 181, 'num_leaves': 109, 'reg_alpha': 0.7515471053585582, 'reg_lambda': 0.23763250176196202, 'n_estimators': 2328, 'colsample_bytree': 0.7804289937571806, 'min_child_samples': 230, 'subsample_freq': 10, 'subsample': 0.7028507225209492, 'max_bin': 925, 'min_data_per_group': 49, 'cat_smooth': 213, 'cat_l2': 5}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:08,369]\u001b[0m Trial 15 finished with value: 0.3711901400601859 and parameters: {'learning_rate': 0.0021362242522428693, 'max_depth': 178, 'num_leaves': 106, 'reg_alpha': 0.9621564632848566, 'reg_lambda': 0.511106575702552, 'n_estimators': 2156, 'colsample_bytree': 0.7073374823936895, 'min_child_samples': 5, 'subsample_freq': 8, 'subsample': 0.8488755274112635, 'max_bin': 989, 'min_data_per_group': 50, 'cat_smooth': 211, 'cat_l2': 4}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:08,941]\u001b[0m Trial 16 finished with value: 2.3109885239723225 and parameters: {'learning_rate': 0.0030643865225366074, 'max_depth': 169, 'num_leaves': 104, 'reg_alpha': 0.8680688906173952, 'reg_lambda': 0.5303120519183627, 'n_estimators': 2049, 'colsample_bytree': 0.7031869102750025, 'min_child_samples': 346, 'subsample_freq': 8, 'subsample': 0.8541314146556174, 'max_bin': 136, 'min_data_per_group': 22, 'cat_smooth': 190, 'cat_l2': 10}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:10,435]\u001b[0m Trial 17 finished with value: 1.0924344317995849 and parameters: {'learning_rate': 0.0018358415990772532, 'max_depth': 84, 'num_leaves': 108, 'reg_alpha': 0.7642934392474227, 'reg_lambda': 0.7164281490776665, 'n_estimators': 1442, 'colsample_bytree': 0.7194709715451825, 'min_child_samples': 195, 'subsample_freq': 8, 'subsample': 0.8439476452812544, 'max_bin': 887, 'min_data_per_group': 6, 'cat_smooth': 205, 'cat_l2': 5}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:12,709]\u001b[0m Trial 18 finished with value: 0.5947684235169114 and parameters: {'learning_rate': 0.003284235141172742, 'max_depth': 86, 'num_leaves': 94, 'reg_alpha': 0.6733187766758992, 'reg_lambda': 0.5421804118404573, 'n_estimators': 2700, 'colsample_bytree': 0.8065562332443162, 'min_child_samples': 54, 'subsample_freq': 5, 'subsample': 0.8907917140728582, 'max_bin': 783, 'min_data_per_group': 43, 'cat_smooth': 165, 'cat_l2': 8}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:15,873]\u001b[0m Trial 19 finished with value: 0.5654012381843888 and parameters: {'learning_rate': 0.001233992486719111, 'max_depth': 161, 'num_leaves': 66, 'reg_alpha': 0.9995428846190686, 'reg_lambda': 0.2671056738782819, 'n_estimators': 2085, 'colsample_bytree': 0.7185422656183083, 'min_child_samples': 4, 'subsample_freq': 9, 'subsample': 0.8621887894377449, 'max_bin': 493, 'min_data_per_group': 11, 'cat_smooth': 116, 'cat_l2': 12}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:17,025]\u001b[0m Trial 20 finished with value: 0.6179751451325529 and parameters: {'learning_rate': 0.0026450187013299124, 'max_depth': 230, 'num_leaves': 113, 'reg_alpha': 0.8832203300390571, 'reg_lambda': 0.6646973805659411, 'n_estimators': 2606, 'colsample_bytree': 0.7844989176539536, 'min_child_samples': 160, 'subsample_freq': 7, 'subsample': 0.8257292042876879, 'max_bin': 946, 'min_data_per_group': 38, 'cat_smooth': 233, 'cat_l2': 4}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:19,241]\u001b[0m Trial 21 finished with value: 0.49129566146030684 and parameters: {'learning_rate': 0.0008033287215033465, 'max_depth': 201, 'num_leaves': 120, 'reg_alpha': 0.9583483364944239, 'reg_lambda': 0.46626720326221094, 'n_estimators': 2461, 'colsample_bytree': 0.7517294464152992, 'min_child_samples': 30, 'subsample_freq': 9, 'subsample': 0.7291713557800591, 'max_bin': 1013, 'min_data_per_group': 50, 'cat_smooth': 223, 'cat_l2': 3}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:21,190]\u001b[0m Trial 22 finished with value: 0.6185050433227981 and parameters: {'learning_rate': 0.003839347786919082, 'max_depth': 198, 'num_leaves': 99, 'reg_alpha': 0.9761597840474932, 'reg_lambda': 0.5296384371348453, 'n_estimators': 2011, 'colsample_bytree': 0.7544182614457551, 'min_child_samples': 53, 'subsample_freq': 9, 'subsample': 0.7312510146099741, 'max_bin': 1018, 'min_data_per_group': 46, 'cat_smooth': 220, 'cat_l2': 7}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:23,445]\u001b[0m Trial 23 finished with value: 0.49011518382732416 and parameters: {'learning_rate': 0.0011747937684461343, 'max_depth': 177, 'num_leaves': 115, 'reg_alpha': 0.6755463149488573, 'reg_lambda': 0.4709657906704272, 'n_estimators': 2536, 'colsample_bytree': 0.7239305459149701, 'min_child_samples': 48, 'subsample_freq': 9, 'subsample': 0.7599067980628756, 'max_bin': 859, 'min_data_per_group': 50, 'cat_smooth': 184, 'cat_l2': 3}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:154: UserWarning: Converting column-vector to 1d array\n  _log_warning('Converting column-vector to 1d array')\n\u001b[32m[I 2021-05-05 06:58:25,425]\u001b[0m Trial 24 finished with value: 0.5580803570095274 and parameters: {'learning_rate': 0.0022993042982546123, 'max_depth': 174, 'num_leaves': 112, 'reg_alpha': 0.6654526307549736, 'reg_lambda': 0.28974654845618236, 'n_estimators': 2997, 'colsample_bytree': 0.7151408202533468, 'min_child_samples': 64, 'subsample_freq': 7, 'subsample': 0.7703568307272237, 'max_bin': 838, 'min_data_per_group': 48, 'cat_smooth': 184, 'cat_l2': 4}. Best is trial 12 with value: 0.3265924903581929.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":["study.best_params\n","\n","# {'learning_rate': 0.0026955685638868463,\n","#  'max_depth': 180,\n","#  'num_leaves': 110,\n","#  'reg_alpha': 0.8365820878758264,\n","#  'reg_lambda': 0.5052568831768142,\n","#  'n_estimators': 2458,\n","#  'colsample_bytree': 0.7589370718114493,\n","#  'min_child_samples': 7,\n","#  'subsample_freq': 10,\n","#  'subsample': 0.7036373549101399,\n","#  'max_bin': 906,\n","#  'min_data_per_group': 50,\n","#  'cat_smooth': 242,\n","#  'cat_l2': 4}"],"metadata":{"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"{'learning_rate': 0.0026955685638868463,\n 'max_depth': 180,\n 'num_leaves': 110,\n 'reg_alpha': 0.8365820878758264,\n 'reg_lambda': 0.5052568831768142,\n 'n_estimators': 2458,\n 'colsample_bytree': 0.7589370718114493,\n 'min_child_samples': 7,\n 'subsample_freq': 10,\n 'subsample': 0.7036373549101399,\n 'max_bin': 906,\n 'min_data_per_group': 50,\n 'cat_smooth': 242,\n 'cat_l2': 4}"},"metadata":{}}]},{"cell_type":"code","source":["lgbm_params = {'learning_rate': 0.0026955685638868463,\n"," 'max_depth': 180,\n"," 'num_leaves': 110,\n"," 'reg_alpha': 0.8365820878758264,\n"," 'reg_lambda': 0.5052568831768142,\n"," 'n_estimators': 2458,\n"," 'colsample_bytree': 0.7589370718114493,\n"," 'min_child_samples': 7,\n"," 'subsample_freq': 10,\n"," 'subsample': 0.7036373549101399,\n"," 'max_bin': 906,\n"," 'min_data_per_group': 50,\n"," 'cat_smooth': 242,\n"," 'cat_l2': 4}"],"metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def objective2(trial, X=X, y=y):\n","    X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25,random_state=meta_random_seed)\n","    \n","    xgb_params={\n","        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n","        'max_depth': trial.suggest_int('max_depth', 6, 31),\n","        'alpha': trial.suggest_float('alpha', 1e-1, 1),\n","        'gamma': trial.suggest_float('gamma', 1e-1, 1),\n","#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-1, 1),\n","        'random_state': meta_random_seed,\n","        'n_estimators': trial.suggest_int('n_estimators', 6, 3000),\n","        'n_jobs': -1,\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.9),\n","        'min_child_weight': trial.suggest_int('min_child_weight', 1, 500),\n","        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n","        'max_bin': trial.suggest_int('max_bin', 128, 1024)       \n","#     params = {\n","#         'n_estimators': trial.suggest_int('n_estimators', 350, 1000),\n","#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n","#         'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.10),\n","#         'subsample': trial.suggest_uniform('subsample', 0.50, 0.90),\n","#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 0.90),\n","#         'gamma': trial.suggest_int('gamma', 0, 20),\n","#         'missing': -999,\n","#         'tree_method': 'gpu_hist'  \n","#     }\n","    }\n","\n","    xgb = XGBRegressor(\n","        **xgb_params,tree_method = 'gpu_hist'\n","    )\n","    xgb.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_test,y_test)],\n","        eval_metric='rmse',\n","        early_stopping_rounds=100,\n","        verbose=False\n","    )\n","    predictions=xgb.predict(X_test)\n","\n","    return mean_absolute_percentage_error(y_test, predictions)"],"metadata":{"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":["study2 = optuna.create_study(direction='minimize')\n","study2.optimize(objective2, timeout=3600*6, n_trials=15)"],"metadata":{"trusted":true},"execution_count":125,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2021-05-05 07:24:56,695]\u001b[0m A new study created in memory with name: no-name-b55a00ba-7d79-4dc4-8814-01c6a652f86c\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:24:57,968]\u001b[0m Trial 0 finished with value: 2.85772015370366 and parameters: {'learning_rate': 0.002222756793278268, 'max_depth': 11, 'alpha': 0.39013442018130384, 'gamma': 0.8049052819100202, 'n_estimators': 1060, 'colsample_bytree': 0.8087221109310607, 'min_child_weight': 386, 'subsample': 0.861113748957127, 'max_bin': 956}. Best is trial 0 with value: 2.85772015370366.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:24:58,393]\u001b[0m Trial 1 finished with value: 1.605848232713437 and parameters: {'learning_rate': 0.004114217790413538, 'max_depth': 9, 'alpha': 0.8452244003245291, 'gamma': 0.508415504017943, 'n_estimators': 256, 'colsample_bytree': 0.5121756988891193, 'min_child_weight': 102, 'subsample': 0.7485576448329493, 'max_bin': 528}. Best is trial 1 with value: 1.605848232713437.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:25:41,429]\u001b[0m Trial 2 finished with value: 1.5741673842914163 and parameters: {'learning_rate': 0.007081687855274977, 'max_depth': 26, 'alpha': 0.5487079399693418, 'gamma': 0.18555189768228691, 'n_estimators': 125, 'colsample_bytree': 0.5659067083044096, 'min_child_weight': 447, 'subsample': 0.8576155149496483, 'max_bin': 690}. Best is trial 2 with value: 1.5741673842914163.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:25:44,039]\u001b[0m Trial 3 finished with value: 1.4704674675073417 and parameters: {'learning_rate': 0.0018800314311429142, 'max_depth': 16, 'alpha': 0.2516299216083434, 'gamma': 0.9382629503388609, 'n_estimators': 1519, 'colsample_bytree': 0.8616778784529846, 'min_child_weight': 132, 'subsample': 0.7089219366903059, 'max_bin': 332}. Best is trial 3 with value: 1.4704674675073417.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:26:25,024]\u001b[0m Trial 4 finished with value: 2.447889674693751 and parameters: {'learning_rate': 0.0063632897698572164, 'max_depth': 22, 'alpha': 0.2700903221108366, 'gamma': 0.4804089874061832, 'n_estimators': 2366, 'colsample_bytree': 0.6511531559722635, 'min_child_weight': 466, 'subsample': 0.7397190369764141, 'max_bin': 308}. Best is trial 3 with value: 1.4704674675073417.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:31:52,143]\u001b[0m Trial 5 finished with value: 2.47255543227048 and parameters: {'learning_rate': 0.0007751157971463761, 'max_depth': 25, 'alpha': 0.6375355248594394, 'gamma': 0.19045879109955915, 'n_estimators': 1940, 'colsample_bytree': 0.7811133346882588, 'min_child_weight': 339, 'subsample': 0.784568435501314, 'max_bin': 747}. Best is trial 3 with value: 1.4704674675073417.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n\u001b[32m[I 2021-05-05 07:31:52,727]\u001b[0m Trial 6 finished with value: 1.6617644950590742 and parameters: {'learning_rate': 0.0020390751309239657, 'max_depth': 16, 'alpha': 0.4231544146656423, 'gamma': 0.21828811363302453, 'n_estimators': 390, 'colsample_bytree': 0.5490795438352813, 'min_child_weight': 350, 'subsample': 0.8351975776479255, 'max_bin': 468}. Best is trial 3 with value: 1.4704674675073417.\u001b[0m\n\u001b[33m[W 2021-05-05 07:31:52,745]\u001b[0m Trial 7 failed because of the following error: XGBoostError('[07:31:52] ../src/tree/updater_gpu_hist.cu:793: Exception in gpu_hist: [07:31:52] ../src/tree/param.h:249: Check failed: n_nodes != 0 (0 vs. 0) : \\nStack trace:\\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x276507) [0x7f997fb10507]\\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4deae1) [0x7f997fd78ae1]\\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e8df4) [0x7f997fd82df4]\\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\\n  [bt] (6) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\\n  [bt] (7) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\\n\\n\\n\\nStack trace:\\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e9478) [0x7f997fd83478]\\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\\n  [bt] (6) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\\n  [bt] (7) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f9a50891067]\\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f4) [0x7f9a508a7794]\\n\\n')\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/optuna/_optimize.py\", line 217, in _run_trial\n    value_or_values = func(trial)\n  File \"<ipython-input-124-75be18d188a2>\", line 38, in objective2\n    verbose=False\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 433, in inner_f\n    return f(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\", line 747, in fit\n    callbacks=callbacks,\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/training.py\", line 197, in train\n    early_stopping_rounds=early_stopping_rounds)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/training.py\", line 81, in _train_internal\n    bst.update(dtrain, i, obj)\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 1498, in update\n    dtrain.handle))\n  File \"/opt/conda/lib/python3.7/site-packages/xgboost/core.py\", line 210, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [07:31:52] ../src/tree/updater_gpu_hist.cu:793: Exception in gpu_hist: [07:31:52] ../src/tree/param.h:249: Check failed: n_nodes != 0 (0 vs. 0) : \nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x276507) [0x7f997fb10507]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4deae1) [0x7f997fd78ae1]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e8df4) [0x7f997fd82df4]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\n  [bt] (6) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\n  [bt] (7) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\n\n\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e9478) [0x7f997fd83478]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\n  [bt] (6) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\n  [bt] (7) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f9a50891067]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f4) [0x7f9a508a7794]\n\n\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-125-d6ccfba27771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-124-75be18d188a2>\u001b[0m in \u001b[0;36mobjective2\u001b[0;34m(trial, X, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         )\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1496\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1497\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mXGBoostError\u001b[0m: [07:31:52] ../src/tree/updater_gpu_hist.cu:793: Exception in gpu_hist: [07:31:52] ../src/tree/param.h:249: Check failed: n_nodes != 0 (0 vs. 0) : \nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x276507) [0x7f997fb10507]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4deae1) [0x7f997fd78ae1]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e8df4) [0x7f997fd82df4]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\n  [bt] (6) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\n  [bt] (7) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\n\n\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e9478) [0x7f997fd83478]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\n  [bt] (6) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\n  [bt] (7) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f9a50891067]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f4) [0x7f9a508a7794]\n\n"],"ename":"XGBoostError","evalue":"[07:31:52] ../src/tree/updater_gpu_hist.cu:793: Exception in gpu_hist: [07:31:52] ../src/tree/param.h:249: Check failed: n_nodes != 0 (0 vs. 0) : \nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x276507) [0x7f997fb10507]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4deae1) [0x7f997fd78ae1]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e8df4) [0x7f997fd82df4]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\n  [bt] (6) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\n  [bt] (7) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\n\n\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x912df) [0x7f997f92b2df]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x4e9478) [0x7f997fd83478]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18c792) [0x7f997fa26792]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x18f07d) [0x7f997fa2907d]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1b9ac3) [0x7f997fa53ac3]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x50) [0x7f997f91ae70]\n  [bt] (6) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f9a508919dd]\n  [bt] (7) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.7(+0x6067) [0x7f9a50891067]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f4) [0x7f9a508a7794]\n\n","output_type":"error"}]},{"cell_type":"code","source":["#study.best_params\n","study2.best_params"],"metadata":{"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6422f3b2db7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#study.best_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'study2' is not defined"],"ename":"NameError","evalue":"name 'study2' is not defined","output_type":"error"}]},{"cell_type":"code","source":["xgb_params = {'learning_rate': 0.0018800314311429142,\n"," 'max_depth': 16,\n"," 'alpha': 0.2516299216083434,\n"," 'gamma': 0.9382629503388609,\n"," 'n_estimators': 1519,\n"," 'colsample_bytree': 0.8616778784529846,\n"," 'min_child_weight': 132,\n"," 'subsample': 0.7089219366903059,\n"," 'max_bin': 332}"],"metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#"],"metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["train_raw2"],"metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"      BåŽ‚  year  month  date  wee       sum       sub       div       log\n0      0     1      1  2018    1 -0.678731 -0.939331  1.059117  1.004726\n1      0     1      2  2018    1 -0.040439 -1.072916  1.034185  1.002742\n2      0     1      3  2018    1  0.166579 -1.089144  1.030923  1.002478\n3      0     1      4  2018    1 -0.047661 -1.187690  1.017847  1.001442\n4      0     1      5  2018    1 -0.359147 -1.013835  0.957121  0.996423\n...   ..   ...    ...   ...  ...       ...       ...       ...       ...\n2065   1    10     27  2020    1  1.475312  0.009076  1.163365  1.012207\n2066   1    10     28  2020    1  1.361221 -0.130746  1.146982  1.011069\n2067   1    10     29  2020    1  1.201056 -0.272881  1.130873  1.009936\n2068   1    10     30  2020    1  1.478330  0.290262  1.201476  1.014828\n2069   1    10     31  2020    0  1.286648 -0.008058  1.165045  1.012347\n\n[2070 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BåŽ‚</th>\n      <th>year</th>\n      <th>month</th>\n      <th>date</th>\n      <th>wee</th>\n      <th>sum</th>\n      <th>sub</th>\n      <th>div</th>\n      <th>log</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>-0.678731</td>\n      <td>-0.939331</td>\n      <td>1.059117</td>\n      <td>1.004726</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>-0.040439</td>\n      <td>-1.072916</td>\n      <td>1.034185</td>\n      <td>1.002742</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>0.166579</td>\n      <td>-1.089144</td>\n      <td>1.030923</td>\n      <td>1.002478</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>-0.047661</td>\n      <td>-1.187690</td>\n      <td>1.017847</td>\n      <td>1.001442</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>-0.359147</td>\n      <td>-1.013835</td>\n      <td>0.957121</td>\n      <td>0.996423</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2065</th>\n      <td>1</td>\n      <td>10</td>\n      <td>27</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>1.475312</td>\n      <td>0.009076</td>\n      <td>1.163365</td>\n      <td>1.012207</td>\n    </tr>\n    <tr>\n      <th>2066</th>\n      <td>1</td>\n      <td>10</td>\n      <td>28</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>1.361221</td>\n      <td>-0.130746</td>\n      <td>1.146982</td>\n      <td>1.011069</td>\n    </tr>\n    <tr>\n      <th>2067</th>\n      <td>1</td>\n      <td>10</td>\n      <td>29</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>1.201056</td>\n      <td>-0.272881</td>\n      <td>1.130873</td>\n      <td>1.009936</td>\n    </tr>\n    <tr>\n      <th>2068</th>\n      <td>1</td>\n      <td>10</td>\n      <td>30</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>1.478330</td>\n      <td>0.290262</td>\n      <td>1.201476</td>\n      <td>1.014828</td>\n    </tr>\n    <tr>\n      <th>2069</th>\n      <td>1</td>\n      <td>10</td>\n      <td>31</td>\n      <td>2020</td>\n      <td>0</td>\n      <td>1.286648</td>\n      <td>-0.008058</td>\n      <td>1.165045</td>\n      <td>1.012347</td>\n    </tr>\n  </tbody>\n</table>\n<p>2070 rows Ã— 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["train_df = X\n","test_df = tests_raw2"],"metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold,RepeatedKFold\n","from scipy.special import expit\n","from sklearn.calibration import CalibratedClassifierCV\n","\n","random_state = 1026529\n","n_folds = 6\n","k_fold = RepeatedKFold(n_splits=n_folds, random_state=random_state)\n","\n","# y = train_df[\"target\"]\n","\n","xgb_train_preds = np.zeros(len(train_df.index), )\n","xgb_test_preds = np.zeros(len(test_df.index), )\n","# xgb_features = xgb_cat_features + list(numerical_columns)\n","\n","lgbm_train_preds = np.zeros(len(train_df.index), )\n","lgbm_test_preds = np.zeros(len(test_df.index), )\n","# lgbm_features = lgb_cat_features + list(numerical_columns)\n","\n","# cb_train_preds = np.zeros(len(train_df.index), )\n","# cb_test_preds = np.zeros(len(test_df.index), )\n","# cb_features = cb_cat_features + list(numerical_columns)\n","\n","# ridge_train_preds = np.zeros(len(train_df.index), )\n","# ridge_test_preds = np.zeros(len(test_df.index), )\n","# ridge_features = ridge_cat_features + list(numerical_columns)"],"metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["for fold, (train_index, test_index) in enumerate(k_fold.split(train_df, y)):\n","    print(\"--> Fold {}\".format(fold + 1))\n","    y_train = y.iloc[train_index]\n","    y_valid = y.iloc[test_index]\n","\n","    ########## Generate train and valid sets ##########\n","    xgb_x_train = pd.DataFrame(train_df.iloc[train_index])\n","    xgb_x_valid = pd.DataFrame(train_df.iloc[test_index])\n","\n","    lgbm_x_train = pd.DataFrame(train_df.iloc[train_index])\n","    lgbm_x_valid = pd.DataFrame(train_df.iloc[test_index])\n","    \n","#     cb_x_train = pd.DataFrame(train_df[cb_features].iloc[train_index])\n","#     cb_x_valid = pd.DataFrame(train_df[cb_features].iloc[test_index])\n","\n","#     ridge_x_train = pd.DataFrame(train_df[ridge_features].iloc[train_index])\n","#     ridge_x_valid = pd.DataFrame(train_df[ridge_features].iloc[test_index])\n","\n","    ########## XGBoost model ##########\n","    xgb_model = XGBRegressor(\n","        seed=random_state,\n","        n_estimators=1519,\n","#         verbosity=1,\n","        eval_metric=\"rmse\",\n","        tree_method=\"gpu_hist\",\n","        gpu_id=0,\n","        alpha=0.2516299216083434,\n","        colsample_bytree=0.8616778784529846,\n","        gamma=0.9382629503388609,\n","        reg_lambda=0.969826765347235612,\n","        learning_rate=0.0018800314311429142,\n","        max_bin=332,\n","        max_depth=16,\n","        min_child_weight=132,\n","        subsample=0.7089219366903059,\n","    )\n","    xgb_model.fit(\n","        xgb_x_train,\n","        y_train,\n","        eval_set=[(xgb_x_valid, y_valid)], \n","        verbose=0,\n","        early_stopping_rounds=200\n","    )\n","\n","    train_oof_preds = xgb_model.predict(xgb_x_valid)\n","    test_oof_preds = xgb_model.predict(test_df)\n","    xgb_train_preds[test_index] = train_oof_preds\n","    xgb_test_preds += test_oof_preds / n_folds\n","    \n","    print(\": XGB - RMSE = {}\".format(mean_absolute_percentage_error(y_valid, train_oof_preds)))\n","    \n","    ########## LGBM model ##########\n","    lgbm_model = LGBMRegressor(\n","        random_state=random_state,\n","        cat_l2=4,\n","        cat_smooth=242,\n","        colsample_bytree=0.7589370718114493,\n","        early_stopping_round=200,\n","        learning_rate=0.0026955685638868463,\n","        max_bin=906,\n","        max_depth=180,\n","        metric=\"rmse\",\n","        min_child_samples=7,\n","        min_data_per_group=50,\n","        n_estimators=2458,\n","        n_jobs=-1,\n","        num_leaves=110,\n","        reg_alpha=0.8365820878758264,\n","        reg_lambda=0.5052568831768142,\n","        subsample=0.7036373549101399,\n","        subsample_freq=10,\n","        verbose=-1,\n","    )\n","    lgbm_model.fit(\n","        lgbm_x_train,\n","        y_train,\n","        eval_set=[(lgbm_x_valid, y_valid)], \n","        verbose=0,\n","    )\n","\n","    train_oof_preds = lgbm_model.predict(lgbm_x_valid)\n","    test_oof_preds = lgbm_model.predict(test_df)\n","    lgbm_train_preds[test_index] = train_oof_preds\n","    lgbm_test_preds += test_oof_preds / n_folds\n","    \n","    print(\": LGB - RMSE = {}\".format(mean_absolute_percentage_error(y_valid, train_oof_preds)))"],"metadata":{"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"--> Fold 1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.41972605943558\n: LGB - ROC AUC Score = 0.28256243377861545\n--> Fold 2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.730985982905113\n: LGB - ROC AUC Score = 0.19142691932569186\n--> Fold 3\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.6742798959572752\n: LGB - ROC AUC Score = 0.18103908681632758\n--> Fold 4\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7943338095805067\n: LGB - ROC AUC Score = 0.22164058554433833\n--> Fold 5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7275864643700751\n: LGB - ROC AUC Score = 0.19646118654692216\n--> Fold 6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.6844159188518613\n: LGB - ROC AUC Score = 0.5990103592901493\n--> Fold 7\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.195131856081139\n: LGB - ROC AUC Score = 0.31760324659903516\n--> Fold 8\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.1447623236870423\n: LGB - ROC AUC Score = 0.5422824814428752\n--> Fold 9\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.6781551761126751\n: LGB - ROC AUC Score = 0.2040114768257249\n--> Fold 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.9591280567142042\n: LGB - ROC AUC Score = 0.20308995920763045\n--> Fold 11\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.8906772415468682\n: LGB - ROC AUC Score = 0.24378868743475152\n--> Fold 12\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.9074807627488626\n: LGB - ROC AUC Score = 0.174812802394348\n--> Fold 13\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.3612079716496988\n: LGB - ROC AUC Score = 0.5149075161247131\n--> Fold 14\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7209051802046655\n: LGB - ROC AUC Score = 0.2643879931801307\n--> Fold 15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.6938824329042103\n: LGB - ROC AUC Score = 0.1386340004204951\n--> Fold 16\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.1600656354657075\n: LGB - ROC AUC Score = 0.15896193798121816\n--> Fold 17\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.0936062530981363\n: LGB - ROC AUC Score = 0.2552468815591721\n--> Fold 18\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.922709928623374\n: LGB - ROC AUC Score = 0.20984585361685487\n--> Fold 19\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7693462545281243\n: LGB - ROC AUC Score = 0.16204654419090628\n--> Fold 20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.9915839913967932\n: LGB - ROC AUC Score = 0.26697195192513873\n--> Fold 21\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.2726844297633555\n: LGB - ROC AUC Score = 0.26233990646359784\n--> Fold 22\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.5887148994511409\n: LGB - ROC AUC Score = 0.4672232958515577\n--> Fold 23\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.501417931617534\n: LGB - ROC AUC Score = 0.11224296457242877\n--> Fold 24\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.8903795976752881\n: LGB - ROC AUC Score = 0.2657220610295274\n--> Fold 25\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7988719706320435\n: LGB - ROC AUC Score = 0.1856101342903482\n--> Fold 26\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.8311258340163523\n: LGB - ROC AUC Score = 0.18056427655809407\n--> Fold 27\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.6518008276729806\n: LGB - ROC AUC Score = 0.1412718724948048\n--> Fold 28\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.0056950358833374\n: LGB - ROC AUC Score = 0.23158481710937556\n--> Fold 29\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.4760696996116516\n: LGB - ROC AUC Score = 0.4972599329327929\n--> Fold 30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.0921132345482871\n: LGB - ROC AUC Score = 0.4411619650444065\n--> Fold 31\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7929759126171506\n: LGB - ROC AUC Score = 0.1892915636875071\n--> Fold 32\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7549186912331173\n: LGB - ROC AUC Score = 0.2603335690770404\n--> Fold 33\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.6873581404576934\n: LGB - ROC AUC Score = 0.45481726584854315\n--> Fold 34\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.6462357179400997\n: LGB - ROC AUC Score = 0.1687818215585598\n--> Fold 35\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.2355385882011176\n: LGB - ROC AUC Score = 0.34385352020495685\n--> Fold 36\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7711172627418149\n: LGB - ROC AUC Score = 0.14844252534795963\n--> Fold 37\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7092406645476133\n: LGB - ROC AUC Score = 0.2567292464401258\n--> Fold 38\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.8923153700325887\n: LGB - ROC AUC Score = 0.15896126747857844\n--> Fold 39\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.2515786311102908\n: LGB - ROC AUC Score = 0.4452468824469767\n--> Fold 40\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.2565690665437323\n: LGB - ROC AUC Score = 0.371912565442734\n--> Fold 41\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.799228104804194\n: LGB - ROC AUC Score = 0.1769041088039496\n--> Fold 42\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.036520417197151\n: LGB - ROC AUC Score = 0.19328758093587026\n--> Fold 43\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.00053901680413\n: LGB - ROC AUC Score = 0.20523038497210594\n--> Fold 44\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.661417168123906\n: LGB - ROC AUC Score = 0.16642218891424432\n--> Fold 45\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.4379837455957076\n: LGB - ROC AUC Score = 0.7356053922109089\n--> Fold 46\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.9591091379094908\n: LGB - ROC AUC Score = 0.30838732535344543\n--> Fold 47\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.1842225762730976\n: LGB - ROC AUC Score = 0.19480211196234407\n--> Fold 48\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7009912735944899\n: LGB - ROC AUC Score = 0.16144232087408772\n--> Fold 49\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7110911742065427\n: LGB - ROC AUC Score = 0.18528787905493088\n--> Fold 50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.2516995897901342\n: LGB - ROC AUC Score = 0.32481545627410174\n--> Fold 51\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.5615122358248288\n: LGB - ROC AUC Score = 0.14828566559251605\n--> Fold 52\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7614185417092442\n: LGB - ROC AUC Score = 0.17398315311731322\n--> Fold 53\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.1051163758433125\n: LGB - ROC AUC Score = 0.2109800588758175\n--> Fold 54\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.536652352751775\n: LGB - ROC AUC Score = 0.49788585978740346\n--> Fold 55\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.7330317951928416\n: LGB - ROC AUC Score = 0.509721813147992\n--> Fold 56\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.8517675869639627\n: LGB - ROC AUC Score = 0.22698167042795853\n--> Fold 57\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7281362341230573\n: LGB - ROC AUC Score = 0.20167051608984615\n--> Fold 58\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 1.1303225217317392\n: LGB - ROC AUC Score = 0.29200407564152026\n--> Fold 59\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.6753780857198867\n: LGB - ROC AUC Score = 0.1684573305735969\n--> Fold 60\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n","output_type":"stream"},{"name":"stdout","text":": XGB - ROC AUC Score = 0.7330610409492407\n: LGB - ROC AUC Score = 0.16540318308129962\n","output_type":"stream"}]},{"cell_type":"code","source":["pd.DataFrame(lgbm_test_preds/n_folds).to_csv('./lgbm55.csv')"],"metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(xgb_test_preds/n_folds).to_csv('./xgb55.csv')"],"metadata":{"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{}},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# over "],"metadata":{}},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import xgboost as xgb\n","\n","    \n","params = {'objective': 'reg:squarederror', \n","              'eta': 0.01, \n","              'max_depth': 16, \n","              'subsample': 0.9, \n","              'colsample_bytree': 1,  \n","              'eval_metric': 'rmse', \n","              'seed': 42, \n","              'silent': True,\n","    }\n","\n","model_xgb = xgb.XGBRegressor(**params, n_estimators=1000,learning_rate=0.05,verbose_eval=True)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objective(trial,data=x,target=y):\n","    \n","    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25,random_state=1026s)\n","    \n","    # To select which parameters to optimize, please look at the XGBoost documentation:\n","    # https://xgboost.readthedocs.io/en/latest/parameter.html\n","    param = {\n","        'device':'gpu',  # Use GPU acceleration\n","        'metric': 'rmse',\n","        'random_state': 42,\n","        'reg_lambda': trial.suggest_loguniform(\n","            'reg_lambda', 1e-3, 10.0\n","        ),\n","        'reg_alpha': trial.suggest_loguniform(\n","            'reg_alpha', 1e-3, 10.0\n","        ),\n","        'colsample_bytree': trial.suggest_categorical(\n","            'colsample_bytree', [0.3,0.5,0.6,0.7,0.8,0.9,1.0]\n","        ),\n","        'subsample': trial.suggest_categorical(\n","            'subsample', [0.6,0.7,0.8,1.0]\n","        ),\n","        'learning_rate': trial.suggest_categorical(\n","            'learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]\n","        ),\n","        'n_estimators': trial.suggest_categorical(\n","            \"n_estimators\", [150, 200, 300, 3000]\n","        ),\n","        'max_depth': trial.suggest_categorical(\n","            'max_depth', [4,5,7,9,11,13,15,17,20]\n","        ),\n","        'min_child_samples': trial.suggest_int(\n","            'min_child_samples', 1, 300\n","        ),\n","        'num_leaves': trial.suggest_int(\n","            'num_leaves', 15, 120\n","        ),\n","    }\n","    model = LGBMRegressor(**param)  \n","    \n","    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=300, verbose=False)\n","    \n","    preds = model.predict(test_x)\n","    \n","    rmse = mean_squared_error(test_y, preds,squared=False)\n","    \n","    return rmse"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","fold_val_pred = []\n","fold_err = []\n","fold_importance = []\n","record = dict()\n","kfold = list(KFold(10,shuffle = True, random_state = 42).split(train_prepared))\n","\n","def xgb_model(train, params, train_label, fold, verbose, pipeline):\n","    for i, (trn, val) in enumerate(fold) :\n","        print(i+1, \"fold.    RMSE\")\n","        trn_x = train.loc[trn, :]\n","        trn_y = train_label[trn]\n","        val_x = train.loc[val, :]\n","        val_y = train_label[val]\n","        fold_val_pred = []\n","    \n","        start = datetime.now()\n","        model = xgb.train(params\n","                      , xgb.DMatrix(trn_x, trn_y)\n","                      , 3500\n","                      , [(xgb.DMatrix(trn_x, trn_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n","                      , verbose_eval=verbose\n","                      , early_stopping_rounds=500\n","                      , callbacks = [xgb.callback.record_evaluation(record)])\n","        best_idx = np.argmin(np.array(record['valid']['rmse']))\n","        val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n","        fold_val_pred.append(val_pred*0.2)\n","        fold_err.append(record['valid_0']['rmse'][best_idx])\n","        fold_importance.append(model.feature_importance('gain'))\n","        \n","        print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelfit(model_xgb, train_prepared, train_labels)\n","\n","#132.587 seconds"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from sklearn.model_selection import GridSearchCV\n","\n","pipe = Pipeline([\n","#     ('pca', PCA(5)),\n","    ('xgb', xgb.XGBRegressor())\n","])\n","\n","param_grid = {\n","#     'pca__n_components': [3, 5, 7, 10,15],\n","    'xgb__n_estimators': [500,1000,2000,3500],\n","    'xgb__learning_rate': [0.09,0.1,0.05,0.001]\n","}\n","\n","pipe.fit(train_prepared, train_labels)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelfit(pipe, train_prepared, train_labels, True)"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import lightgbm as lgb\n","import xgboost as xgb\n","\n","lgb_model = lgb.LGBMRegressor(objective='regression', \n","                                       num_leaves=500,\n","                                       learning_rate=0.01, \n","                                       n_estimators=3000,\n","                                       max_depth = 15,\n","#                                        max_bin=200, \n","                                       bagging_fraction=0.85,\n","                                       bagging_freq=5, \n","                                       bagging_seed=75,\n","#                                        feature_fraction=1,\n","#                                        feature_fraction_seed=75,\n","                                       metric='rmse',\n","                                       verbose=-1,\n","                                       )\n","\n","modelfit(lgb_model, train_prepared, train_labels)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from mlxtend.regressor import StackingCVRegressor\n","models_total = (lin_reg, forest_reg, ada_reg, gdb_model, lgb_model)\n","# model_ada_gbd = (ada_reg,gdb_model)\n","stack_gen = StackingCVRegressor(regressors= models_total,\n","                                meta_regressor=model_xgb,\n","                                use_features_in_secondary=True)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelfit(stack_gen, train_prepared, train_labels)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tests_labels=tests.pop('AåŽ‚')\n","tests_prepared=tests"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_some_regressors = stack_gen.predict(tests_prepared)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_some_regressors"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(prediction_some_regressors).to_csv(\"./kag.csv\", index = False)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]}]}